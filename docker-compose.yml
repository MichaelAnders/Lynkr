version: "3.9"

services:
  # Lynkr proxy service
  lynkr:
    build: .
    container_name: lynkr
    ports:
      - "8080:8080"
      - "8888:8888"
    environment:
      # ============================================================
      # PRIMARY MODEL PROVIDER
      # ============================================================
      # Options: ollama, databricks, azure-anthropic, openrouter
      MODEL_PROVIDER: ${MODEL_PROVIDER:-ollama}

      # ============================================================
      # TOOL EXECUTION MODE
      # ============================================================
      # Options: server (default), client (passthrough mode)
      # - server: Tools execute on proxy server
      # - client: Tools execute on Claude Code CLI (client-side)
      TOOL_EXECUTION_MODE: ${TOOL_EXECUTION_MODE:-server}

      # ============================================================
      # OLLAMA CONFIGURATION
      # ============================================================
      PREFER_OLLAMA: ${PREFER_OLLAMA:-true}
      OLLAMA_ENDPOINT: http://ollama:11434
      OLLAMA_MODEL: ${OLLAMA_MODEL:-qwen2.5-coder:latest}
      OLLAMA_MAX_TOOLS_FOR_ROUTING: ${OLLAMA_MAX_TOOLS_FOR_ROUTING:-3}

      # ============================================================
      # OPENROUTER CONFIGURATION
      # ============================================================
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}
      OPENROUTER_MODEL: ${OPENROUTER_MODEL:-amazon/nova-2-lite-v1:free}
      OPENROUTER_ENDPOINT: ${OPENROUTER_ENDPOINT:-https://openrouter.ai/api/v1/chat/completions}
      OPENROUTER_MAX_TOOLS_FOR_ROUTING: ${OPENROUTER_MAX_TOOLS_FOR_ROUTING:-15}

      # ============================================================
      # HYBRID ROUTING & FALLBACK
      # ============================================================
      # Enable/disable fallback to cloud providers
      FALLBACK_ENABLED: ${FALLBACK_ENABLED:-true}
      # Fallback provider when Ollama can't handle request
      FALLBACK_PROVIDER: ${FALLBACK_PROVIDER:-databricks}

      # ============================================================
      # DATABRICKS CONFIGURATION
      # ============================================================
      DATABRICKS_API_BASE: ${DATABRICKS_API_BASE:-https://example.cloud.databricks.com}
      DATABRICKS_API_KEY: ${DATABRICKS_API_KEY:-replace-with-databricks-pat}

      # ============================================================
      # AZURE ANTHROPIC CONFIGURATION (OPTIONAL)
      # ============================================================
      AZURE_ANTHROPIC_ENDPOINT: ${AZURE_ANTHROPIC_ENDPOINT:-}
      AZURE_ANTHROPIC_API_KEY: ${AZURE_ANTHROPIC_API_KEY:-}

      # ============================================================
      # SERVER CONFIGURATION
      # ============================================================
      PORT: ${PORT:-8080}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      WEB_SEARCH_ENDPOINT: ${WEB_SEARCH_ENDPOINT:-http://localhost:8888/search}
      WORKSPACE_ROOT: /workspace
    volumes:
      - ./data:/app/data  # Persist SQLite databases
      - .:/workspace      # Mount workspace
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - lynkr-network

  # Ollama service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama  # Persist downloaded models
    restart: unless-stopped
    networks:
      - lynkr-network
    # Uncomment for NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Optional: Ollama Web UI (if you want a visual interface)
  # ollama-webui:
  #   image: ghcr.io/open-webui/open-webui:main
  #   container_name: ollama-webui
  #   ports:
  #     - "3000:8080"
  #   environment:
  #     OLLAMA_BASE_URL: http://ollama:11434
  #   volumes:
  #     - ollama-webui-data:/app/backend/data
  #   depends_on:
  #     - ollama
  #   restart: unless-stopped
  #   networks:
  #     - lynkr-network

volumes:
  ollama-data:
    driver: local
  # ollama-webui-data:
  #   driver: local

networks:
  lynkr-network:
    driver: bridge
